#!/data/data/com.termux/files/usr/bin/python3
import telebot
import requests
import os
from dotenv import load_dotenv

load_dotenv()

bot = telebot.TeleBot(os.getenv('TELEGRAM_TOKEN'))
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"

# üéØ MOD√àLES CORRECTS GROQ :
AVAILABLE_MODELS = {
    "llama3-8b": "llama3-8b-8192",
    "llama3-70b": "llama3-70b-8192", 
    "mixtral": "mixtral-8x7b-32768",
    "gemma": "gemma-7b-it"
}

# Mod√®le par d√©faut (choisissez-en un)
SELECTED_MODEL = AVAILABLE_MODELS["llama3-70b"]

@bot.message_handler(commands=['start'])
def start(message):
    welcome_text = f"""
ü§ñ **Bot Groq IA - Mod√®les Corrig√©s !** ‚ö°

üéØ **Commandes :**
/start - D√©marrer
/models - Changer de mod√®le
/test - Test de connexion
/current - Mod√®le actuel

üß† **Mod√®le actuel :** {SELECTED_MODEL}
‚ö° **Vitesse :** R√©ponses ultra-rapides

üí¨ **Posez-moi n'importe quelle question !**
    """
    bot.reply_to(message, welcome_text)

@bot.message_handler(commands=['models'])
def models_command(message):
    models_text = f"""
üß† **Mod√®les Groq Disponibles :**

1. **llama3-70b-8192** (recommand√©)
   - Llama 3 70B dernier cri
   - Tr√®s intelligent
   - Bon en tout

2. **llama3-8b-8192**
   - Llama 3 8B rapide
   - L√©ger et efficace
   - Parfait pour le chat

3. **mixtral-8x7b-32768**
   - 8 experts Mixtral
   - Excellente qualit√©
   - Contexte long

4. **gemma-7b-it** 
   - Mod√®le Google
   - L√©ger et rapide

üîß **Actuel :** {SELECTED_MODEL}
üí° **Changer :** /llama3-70b /llama3-8b /mixtral /gemma
    """
    bot.reply_to(message, models_text)

@bot.message_handler(commands=['llama3-70b', 'llama3-8b', 'mixtral', 'gemma'])
def change_model(message):
    global SELECTED_MODEL
    
    model_command = message.text[1:]  # Enlever le /
    if model_command in AVAILABLE_MODELS:
        SELECTED_MODEL = AVAILABLE_MODELS[model_command]
        bot.reply_to(message, f"‚úÖ Mod√®le chang√© pour : {SELECTED_MODEL}")
    else:
        bot.reply_to(message, "‚ùå Commande de mod√®le invalide")

@bot.message_handler(commands=['current'])
def current_model(message):
    bot.reply_to(message, f"üß† Mod√®le actuel : {SELECTED_MODEL}")

@bot.message_handler(commands=['test'])
def test_command(message):
    try:
        bot.send_chat_action(message.chat.id, 'typing')
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {GROQ_API_KEY}"
        }

        payload = {
            "messages": [
                {
                    "role": "user", 
                    "content": "R√©ponds UNIQUEMENT par '‚úÖ Test r√©ussi avec [MOD√àLE]' en rempla√ßant [MOD√àLE] par le mod√®le utilis√©."
                }
            ],
            "model": SELECTED_MODEL,
            "max_tokens": 50
        }

        response = requests.post(GROQ_API_URL, json=payload, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            answer = data["choices"][0]["message"]["content"]
            bot.reply_to(message, f"üß™ {answer}\n\nüöÄ API Groq fonctionne !")
        else:
            bot.reply_to(message, f"‚ùå Erreur {response.status_code}: {response.text}")
            
    except Exception as e:
        bot.reply_to(message, f"‚ùå Erreur test: {str(e)}")

@bot.message_handler(func=lambda message: True)
def handle_message(message):
    try:
        bot.send_chat_action(message.chat.id, 'typing')
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {GROQ_API_KEY}"
        }

        payload = {
            "messages": [
                {
                    "role": "system", 
                    "content": "Tu es un assistant IA utile. R√©ponds en fran√ßais de mani√®re claire et concise. Sois direct dans tes r√©ponses."
                },
                {
                    "role": "user", 
                    "content": message.text
                }
            ],
            "model": SELECTED_MODEL,
            "temperature": 0.7,
            "max_tokens": 1024,
            "top_p": 1,
            "stream": False
        }

        response = requests.post(GROQ_API_URL, json=payload, headers=headers, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            answer = data["choices"][0]["message"]["content"]
            bot.reply_to(message, answer)
            
        else:
            error_info = f"""
‚ùå **Erreur Groq API**

Code: {response.status_code}
Message: {response.text}

Mod√®le utilis√©: {SELECTED_MODEL}
üîß Essayez /models pour changer de mod√®le
            """
            bot.reply_to(message, error_info)

    except requests.exceptions.Timeout:
        bot.reply_to(message, "‚è∞ Timeout - R√©essayez!")
        
    except Exception as e:
        bot.reply_to(message, f"‚ùå Erreur: {str(e)}")

print("üöÄ Bot Groq d√©marr√© avec mod√®les corrig√©s!")
print(f"üß† Mod√®le par d√©faut: {SELECTED_MODEL}")
print(f"üîë Token Telegram: {'‚úÖ' if os.getenv('TELEGRAM_TOKEN') else '‚ùå'}")
print(f"‚ö° Cl√© Groq: {'‚úÖ' if GROQ_API_KEY else '‚ùå'}")

bot.infinity_polling()
