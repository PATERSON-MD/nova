#!/data/data/com.termux/files/usr/bin/python3
import telebot
import requests
import os
from dotenv import load_dotenv

load_dotenv()

bot = telebot.TeleBot(os.getenv('TELEGRAM_TOKEN'))
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"

# üéØ MOD√àLES GROQ 2025 - TOUJOURS ACTUALIS√âS :
AVAILABLE_MODELS = {
    # ‚úÖ MOD√àLES CONFIRM√âS 2024-2025
    "llama3.1-70b": "llama-3.1-70b-versatile",
    "llama3.1-8b": "llama-3.1-8b-instant", 
    "mixtral": "mixtral-8x7b-32768",
    "gemma2": "gemma2-9b-it",
    
    # üîÆ MOD√àLES ATTENDUS 2025 (√† tester)
    "llama3.2-70b": "llama-3.2-70b",  # Peut-√™tre disponible bient√¥t
    "llama3.2-8b": "llama-3.2-8b",
    "qwen2-72b": "qwen2-72b-instruct",  # Nouveaux mod√®les chinois
}

# Fonction pour d√©tecter automatiquement les mod√®les disponibles
def detect_available_models():
    available = {}
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # Mod√®les √† tester
    test_models = [
        "llama-3.1-70b-versatile",
        "llama-3.1-8b-instant",
        "mixtral-8x7b-32768",
        "gemma2-9b-it",
        "llama-3.2-70b",  # Futur
        "llama-3.2-8b",   # Futur
        "qwen2-72b-instruct"  # Futur
    ]
    
    for model in test_models:
        try:
            payload = {
                "messages": [{"role": "user", "content": "Test"}],
                "model": model,
                "max_tokens": 5
            }
            response = requests.post(GROQ_API_URL, json=payload, headers=headers, timeout=5)
            if response.status_code == 200:
                # Nom court pour les commandes
                short_name = model.split('-')[0] + model.split('-')[1]
                available[short_name] = model
                print(f"‚úÖ Mod√®le d√©tect√©: {model}")
        except:
            continue
    
    return available

# D√©tection automatique au d√©marrage
print("üîç D√©tection des mod√®les Groq 2025...")
ACTIVE_MODELS = detect_available_models()

# Si aucun mod√®le d√©tect√©, utiliser les garantis
if not ACTIVE_MODELS:
    ACTIVE_MODELS = {
        "llama3170b": "llama-3.1-70b-versatile",
        "llama318b": "llama-3.1-8b-instant",
        "mixtral": "mixtral-8x7b-32768"
    }
    print("‚ö†Ô∏è  Utilisation des mod√®les par d√©faut")

SELECTED_MODEL = list(ACTIVE_MODELS.values())[0]  # Premier mod√®le disponible

@bot.message_handler(commands=['start'])
def start(message):
    welcome_text = f"""
ü§ñ **Bot Groq IA - √âdition 2025 !** üéâ

üéØ **Commandes :**
/start - D√©marrer
/models - Mod√®les disponibles  
/test - Test de connexion
/scan - Scanner nouveaux mod√®les
/current - Mod√®le actuel

üß† **Mod√®le actuel :** {SELECTED_MODEL}
‚ö° **Vitesse :** R√©ponses ultra-rapides
üîÆ **IA 2025 :** Derni√®re g√©n√©ration

üí¨ **Posez-moi n'importe quelle question !**
    """
    bot.reply_to(message, welcome_text)

@bot.message_handler(commands=['scan'])
def scan_models(message):
    bot.reply_to(message, "üîç Scan des nouveaux mod√®les Groq...")
    global ACTIVE_MODELS, SELECTED_MODEL
    
    ACTIVE_MODELS = detect_available_models()
    
    if ACTIVE_MODELS:
        models_list = "\n".join([f"‚Ä¢ {name} -> {model}" for name, model in ACTIVE_MODELS.items()])
        bot.reply_to(message, f"‚úÖ Mod√®les d√©tect√©s:\n{models_list}")
    else:
        bot.reply_to(message, "‚ùå Aucun mod√®le d√©tect√©")

@bot.message_handler(commands=['models'])
def models_command(message):
    models_text = f"""
üß† **Mod√®les Groq 2025 Disponibles :**

"""
    
    for short_name, full_model in ACTIVE_MODELS.items():
        models_text += f"‚Ä¢ **{short_name}** -> {full_model}\n"
    
    models_text += f"""
üîß **Actuel :** {SELECTED_MODEL}
üí° **Changer :** /{ " /".join(ACTIVE_MODELS.keys())}
üîÑ **Scanner :** /scan pour nouveaux mod√®les
    """
    bot.reply_to(message, models_text)

# G√©n√©rer dynamiquement les handlers pour chaque mod√®le
for model_short in ACTIVE_MODELS.keys():
    @bot.message_handler(commands=[model_short])
    def change_model_dynamic(message, model_short=model_short):
        global SELECTED_MODEL
        SELECTED_MODEL = ACTIVE_MODELS[model_short]
        bot.reply_to(message, f"‚úÖ Mod√®le chang√© pour : {SELECTED_MODEL}")

@bot.message_handler(commands=['current'])
def current_model(message):
    bot.reply_to(message, f"üß† Mod√®le actuel : {SELECTED_MODEL}")

@bot.message_handler(commands=['test'])
def test_command(message):
    try:
        bot.send_chat_action(message.chat.id, 'typing')
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {GROQ_API_KEY}"
        }

        payload = {
            "messages": [
                {
                    "role": "user", 
                    "content": f"R√©ponds UNIQUEMENT par '‚úÖ 2025 - Mod√®le {SELECTED_MODEL} op√©rationnel !'"
                }
            ],
            "model": SELECTED_MODEL,
            "max_tokens": 50,
            "temperature": 0.1
        }

        response = requests.post(GROQ_API_URL, json=payload, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            answer = data["choices"][0]["message"]["content"]
            bot.reply_to(message, f"üß™ {answer}\n\nüöÄ Pr√™t pour 2025 !")
        else:
            bot.reply_to(message, f"‚ùå Erreur {response.status_code}: {response.text}")
            
    except Exception as e:
        bot.reply_to(message, f"‚ùå Erreur test: {str(e)}")

@bot.message_handler(func=lambda message: True)
def handle_message(message):
    try:
        bot.send_chat_action(message.chat.id, 'typing')
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {GROQ_API_KEY}"
        }

        payload = {
            "messages": [
                {
                    "role": "system", 
                    "content": "Tu es un assistant IA de pointe 2025. R√©ponds en fran√ßais de mani√®re claire, concise et moderne. Sois direct et utile."
                },
                {
                    "role": "user", 
                    "content": message.text
                }
            ],
            "model": SELECTED_MODEL,
            "temperature": 0.7,
            "max_tokens": 1024,
            "top_p": 0.9,
            "stream": False
        }

        response = requests.post(GROQ_API_URL, json=payload, headers=headers, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            answer = data["choices"][0]["message"]["content"]
            bot.reply_to(message, answer)
            
        else:
            error_info = f"""
‚ùå **Erreur API 2025**

Code: {response.status_code}
Message: {response.text}

Mod√®le: {SELECTED_MODEL}
üîß Essayez:
/models - pour changer
/scan - nouveaux mod√®les
/test - diagnostiquer
            """
            bot.reply_to(message, error_info)

    except requests.exceptions.Timeout:
        bot.reply_to(message, "‚è∞ Timeout - IA 2025 trop demand√©e!")
        
    except Exception as e:
        bot.reply_to(message, f"‚ùå Erreur: {str(e)}")

print("üöÄ Bot Groq 2025 - Intelligence Nouvelle G√©n√©ration!")
print(f"üß† Mod√®les d√©tect√©s: {len(ACTIVE_MODELS)}")
print(f"üîë Token Telegram: {'‚úÖ' if os.getenv('TELEGRAM_TOKEN') else '‚ùå'}")
print(f"‚ö° Cl√© Groq: {'‚úÖ' if GROQ_API_KEY else '‚ùå'}")

bot.infinity_polling()
